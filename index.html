<!DOCTYPE html>
<html>
<head>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
  
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="open-set, 3D instance segmentation, multimodal">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DiET-GS</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
        .container2 {
            justify-content: center;
            display: flex;
            flex-direction: row; /* Horizontal alignment of videos */
            gap: 30px; /* Space between videos */
            text-align: center;
        }
        .video-box {
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        video {
            width: 300px; /* Adjust width as needed */
            height: auto;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
    </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">DiET-GS ðŸ«¨<br/>
            <p class="title is-3 publication-title">Diffusion Prior and Event Stream-Assisted <br/> Motion Deblurring 3D Gaussian Splatting</p>
          </h1>
          <h1 class="title is-4" style="color: #5c5c5c;">arXiv 2024</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/seungjun-lee-43101a261/">Seungjun Lee</a>,</span>
            <span class="author-block">
              <a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block" style="margin-right: 1em;">Department of Computer Science, National University of Singapore</span><br>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered">

      <!-- Affordances 1 -->
      <div class="column">
        <div class="content"> 
          <video autoplay loop muted>
                <source src="./static/videos/events.MP4" type="video/mp4">
            </video>
            <center>
            <p>
              Event stream
            </p>
          </center>
        </div>
      </div>
      <div class="column">
        <div class="content"> 
          <img src="static/images/blur.png" class="interpolation-image">
            <center>
            <p>
              Blur image
            </p>
          </center>
        </div>
      </div>
      <div class="column">
        <div class="content"> 
          <img src="static/images/output.png" class="interpolation-image">
            <center>
            <p>
              Output
            </p>
          </center>
        </div>
      </div>
      </div>
      <br>
      <h2 class="subtitle has-text-centered">
     Our <strong>DiET-GS++</strong> enables high quality novel-view synthesis with recovering precise color and well-defined details from the blurry multi-view images.
    </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container2">
      <div class="video-box">
            <video autoplay loop muted>
                <source src="./static/videos/events.MP4" type="video/mp4">
            </video>
            <center>
            <p>
              Event stream
            </p>
          </center>
        </div>
        <div class="video-box">
            <video autoplay loop muted>
                <source src="./static/videos/events.MP4" type="video/mp4">
            </video>
            <center>
            <p>
              Blur Image
            </p>
          </center>
        </div>
        <div class="video-box">
            <video autoplay loop muted>
                <source src="./static/videos/events.MP4" type="video/mp4">
            </video>
            <center>
            <p>
              Output
            </p>
          </center>
        </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          Reconstructing sharp 3D representations from blurry multi-view images are long-standing problem in computer vision. 
          Recent works attempt to enhance high-quality novel view synthesis from the motion blur by leveraging event-based cameras, benefiting from high dynamic range and microsecond temporal resolution. 
          However, they often reach sub-optimal visual quality in either restoring inaccurate color or losing fine-grained details. 
          In this paper, we present <strong>DiET-GS</strong>, a diffusion prior and event stream-assisted motion deblurring 3DGS. 
          Our framework effectively leverages both blur-free event streams and diffusion prior in a two-stage training strategy. 
          Specifically, we introduce the novel framework to constraint 3DGS with event double integral, achieving both accurate color and well-defined details. 
          Additionally, we propose a simple technique to leverage diffusion prior to further enhance the edge details. 
          Qualitative and quantitative results on both synthetic and real-world data demonstrate that our DiET-GS is capable of producing significantly better quality of novel views compared to the existing baselines. 
          The code will be publicly available.
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
       
        <h2 class="title is-3">Overall Framework</h2>
    
      <br>
        <div class="content has-text-justified">
          <p>
            <strong>Overall framework of SOLE.</strong> SOLE is built on transformer-based instance segmentation model with multimodal adaptations. 
            For model architecture, backbone features are integrated with per-point CLIP features and subsequently fed into the cross-modality decoder (CMD). 
            CMD aggregates the point-wise features and textual features into the instance queries, finally segmenting the instances, which are supervised by multimodal associations. 
            During inference, predicted mask features are combined with the per-point CLIP features, enhancing the open-vocabulary performance.
          </p>
        </div>
      </div>
    </div>

    <div class="column">
        <div class="content">
          <img src="static/images/overall.png"
            class="interpolation-image">
        </div>
      </div>

<div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
    
      <br>
        <div class="content has-text-justified">
          <p>
            <strong>Three types of multimodal association instance.</strong> For each ground truth instance mask, we first pool the per-point CLIP features to obtain Mask-Visual Association $\mathbf{f}^{\mathrm{MVA}}$. 
            Subsequently, $\mathbf{f}^{\mathrm{MVA}}$ is fed into CLIP space captioning model to generate caption and corresponding textual feature $\mathbf{f}^{\mathrm{MCA}}$ for each mask, termed as Mask-Caption Association. 
            Finally, noun phrases are extracted from mask caption and the embeddings of them are aggregated via multimodal attention to get Mask-Entity Association $\mathbf{f}^{\mathrm{MEA}}$. 
            The three multimodal associations are used for supervising SOLE to acquire the ability to segment 3D objects with free-form language instructions.
          </p>
        </div>
      </div>
    </div>

    <div class="column">
        <div class="content">
          <img src="static/images/mla.png"
            class="interpolation-image">
        </div>
      </div>
</div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
       
        <h2 class="title is-3">Quantitative Results</h2>
    
      <br>
        <div class="content has-text-justified">
          <p>
            <strong>The comparison of closed-set 3D instance segmentation setting on ScanNetv2.</strong> SOLE is compared with class-split methods, mask-training methods and the full-supervised counterpart (upper bound). 
            SOLE outperforms all the OV-DIS methods and achieves competitive results with the fully-supervised model.  
          </p>
        </div>
      </div>
    </div>

    <div class="column">
        <div class="content">
          <img src="static/images/tab1.png"
            class="interpolation-image">
        </div>
      </div>
<div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
    
      <br>
        <div class="content has-text-justified">
          <p>
            <strong>The comparison of closed-set 3D instance segmentation setting on ScanNet200.</strong> SOLE is compared with OpenMask3D on the overall segmentation performance and on each subset. 
            SOLE significantly outperforms OpenMask3D on five out of the six evaluation metrics.
          </p>
        </div>
      </div>
    </div>

    <div class="column">
        <div class="content">
          <img src="static/images/tab2.png"
            class="interpolation-image">
        </div>
      </div>
<div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
    
      <br>
        <div class="content has-text-justified">
          <p>
            <strong>The comparison of hierarchical open-set 3D instance segmentation setting on ScanNetv2â†’ScanNet200.</strong> 
            SOLE is compared with OpenMask3D on both base and novel classes and achieves the best results.
          </p>
        </div>
      </div>
    </div>

    <div class="column">
        <div class="content">
          <img src="static/images/tab3.png"
            class="interpolation-image">
        </div>
      </div>
  
<div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
    
      <br>
        <div class="content has-text-justified">
          <p>
            <strong>The comparison of open-set 3D instance segmentation setting on ScanNet200â†’Replica.</strong>
            SOLE outperforms OpenMask3D on all the evaluation metrics.
          </p>
        </div>
      </div>
    </div>

    <div class="column">
        <div class="content">
          <img src="static/images/tab4.png"
            class="interpolation-image">
        </div>
      </div>
</div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
       
        <h2 class="title is-3">Qualitative results</h2>
    
      <br>
        <div class="content has-text-justified">
          <p>
            Our <strong>SOLE</strong> demonstrates open-vocabulary capability by effectively responding to free-form language queries, including visual questions, attributes description and functional description.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered">

      <!-- Affordances 1 -->
      <div class="column">
        <div class="content">
          
          <img src="static/images/tv.png"
            class="interpolation-image">
            <center>
            <p>
              <i>I want to watch movie.</i>
            </p>
          </center>
        </div>
      </div>
      <!--/ Affordances 1 -->

      <!-- Affordances 2 -->
      <div class="column">
        <div class="content">
          
          <img src="static/images/window.png"
            class="interpolation-image">
            <center>
            <p>
              <i>I wanna see outside.</i>
            </p>
          </center>
        </div>
      </div>
    <!--/ Affordances 2 -->
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <!-- Affordances 1 -->
      <div class="column">
        <div class="content">
          
          <img src="static/images/chairs.png"
            class="interpolation-image">
            <center>
            <p>
              <i>Chairs near by the window.</i>
            </p>
          </center>
        </div>
      </div>
      <!--/ Affordances 1 -->

      <!-- Affordances 2 -->
      <div class="column">
        <div class="content">
          
          <img src="static/images/brown.png"
            class="interpolation-image">
            <center>
            <p>
              <i>Brown furnitures.</i>
            </p>
          </center>
        </div>
      </div>
    <!--/ Affordances 2 -->
  </div>
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <!-- Affordances 1 -->
      <div class="column">
        <div class="content">
          
          <img src="static/images/trashcan.png"
            class="interpolation-image">
            <center>
            <p>
              <i>Throwing away the garbage.</i>
            </p>
          </center>
        </div>
      </div>
      <!--/ Affordances 1 -->

      <!-- Affordances 2 -->
      <div class="column">
        <div class="content">
          
          <img src="static/images/vending.png"
            class="interpolation-image">
            <center>
            <p>
              <i>I'm hungry.</i>
            </p>
          </center>
        </div>
      </div>
    <!--/ Affordances 2 -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            It borrows the source code of <a href="https://github.com/nerfies/nerfies.github.io">this website</a>.
            We would like to thank Utkarsh Sinha and Keunhong Park.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
